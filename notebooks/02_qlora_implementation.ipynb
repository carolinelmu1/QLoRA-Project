{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Diagnostic Analysis - Part 2: QLoRA (4-bit) Implementation with Unsloth\n",
    "\n",
    "## Objective\n",
    "Implement QLoRA with 4-bit NF4 quantization using **Unsloth** and compare against the 16-bit LoRA baseline from Part 1.\n",
    "\n",
    "## Key Questions\n",
    "1. How much memory does 4-bit quantization save compared to 16-bit?\n",
    "2. Does QLoRA preserve performance (comparable training loss)?\n",
    "3. What is the optimal rank for QLoRA?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "### 1.1 Install Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth - optimized LoRA/QLoRA library\n",
    "import torch\n",
    "\n",
    "# Check CUDA version\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "print(f\"GPU Compute Capability: {major_version}.{minor_version}\")\n",
    "\n",
    "# Install Unsloth\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# Additional dependencies\n",
    "!pip install -q datasets matplotlib seaborn pandas numpy scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Import Unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Add src to path (upload src/ folder to Colab first)\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom modules with clean names\n",
    "from model_utils import load_gpt2_unsloth, setup_gpt2_lora, clear_memory\n",
    "from training import prepare_alpaca_dataset, run_experiment_unsloth\n",
    "from visualization import create_results_table\n",
    "\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental configuration\n",
    "MODEL_NAME = \"gpt2-medium\"  # 355M parameters\n",
    "NUM_SAMPLES = 1000  # Match baseline\n",
    "MAX_STEPS = 200\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# Ranks to test (match baseline)\n",
    "RANKS_TO_TEST = [2, 4, 8, 16]\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"./results_qlora\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Library: Unsloth (optimized)\")\n",
    "print(f\"  Quantization: 4-bit NF4 (QLoRA)\")\n",
    "print(f\"  Training samples: {NUM_SAMPLES}\")\n",
    "print(f\"  Max steps: {MAX_STEPS}\")\n",
    "print(f\"  Ranks to test: {RANKS_TO_TEST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Baseline Results for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline LoRA results\n",
    "try:\n",
    "    with open('../results_baseline_lora/baseline_results.pkl', 'rb') as f:\n",
    "        baseline_results = pickle.load(f)\n",
    "    print(f\"‚úì Loaded {len(baseline_results)} baseline results\")\n",
    "    baseline_df = pd.DataFrame(baseline_results)\n",
    "    print(\"\\nBaseline Summary:\")\n",
    "    display(baseline_df[['rank', 'peak_memory_mb', 'time_per_step', 'training_loss']])\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è  Baseline results not found. Run 01_baseline_lora.ipynb first.\")\n",
    "    baseline_results = None\n",
    "    baseline_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run QLoRA Experiments\n",
    "\n",
    "Train QLoRA (4-bit quantized base + high-precision adapters) with different ranks using Unsloth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "qlora_results_list = []\n",
    "\n",
    "for rank in RANKS_TO_TEST:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running QLoRA (4-bit) with rank r={rank} using Unsloth\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    try:\n",
    "        result, model, tokenizer = run_experiment_unsloth(\n",
    "            model_name=MODEL_NAME,\n",
    "            load_in_4bit=True,  # QLoRA: 4-bit quantization\n",
    "            rank=rank,\n",
    "            num_samples=NUM_SAMPLES,\n",
    "            max_steps=MAX_STEPS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            output_dir=OUTPUT_DIR\n",
    "        )\n",
    "        \n",
    "        qlora_results_list.append(result)\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        del tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with rank {rank}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n‚úì All QLoRA experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Analysis\n",
    "\n",
    "### 5.1 Create Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create QLoRA results table\n",
    "qlora_df = create_results_table(\n",
    "    qlora_results_list,\n",
    "    save_path=f\"{OUTPUT_DIR}/qlora_results.csv\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìä QLORA RESULTS (Unsloth)\")\n",
    "print(\"=\"*80)\n",
    "display(qlora_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Compare LoRA vs QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline_results:\n",
    "    # Combine results\n",
    "    combined_df = pd.concat([baseline_df, qlora_df], ignore_index=True)\n",
    "    \n",
    "    # Calculate memory reduction\n",
    "    comparison = pd.DataFrame()\n",
    "    for rank in RANKS_TO_TEST:\n",
    "        lora_mem = baseline_df[baseline_df['rank'] == rank]['peak_memory_mb'].values[0]\n",
    "        qlora_mem = qlora_df[qlora_df['rank'] == rank]['peak_memory_mb'].values[0]\n",
    "        reduction = ((lora_mem - qlora_mem) / lora_mem) * 100\n",
    "        \n",
    "        lora_loss = baseline_df[baseline_df['rank'] == rank]['training_loss'].values[0]\n",
    "        qlora_loss = qlora_df[qlora_df['rank'] == rank]['training_loss'].values[0]\n",
    "        \n",
    "        comparison = pd.concat([comparison, pd.DataFrame({\n",
    "            'rank': [rank],\n",
    "            'lora_memory_mb': [lora_mem],\n",
    "            'qlora_memory_mb': [qlora_mem],\n",
    "            'memory_reduction_%': [reduction],\n",
    "            'lora_loss': [lora_loss],\n",
    "            'qlora_loss': [qlora_loss]\n",
    "        })], ignore_index=True)\n",
    "    \n",
    "    print(\"\\nüîã MEMORY COMPARISON: LoRA vs QLoRA (Unsloth)\")\n",
    "    print(\"=\"*80)\n",
    "    display(comparison)\n",
    "    \n",
    "    print(f\"\\n‚ú® Average memory reduction: {comparison['memory_reduction_%'].mean():.2f}%\")\n",
    "    print(f\"‚ú® Unsloth optimization benefit: Faster training + reduced memory!\")\n",
    "    \n",
    "    # Save comparison\n",
    "    os.makedirs('../results/tables', exist_ok=True)\n",
    "    comparison.to_csv('../results/tables/memory_comparison.csv', index=False)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Baseline results not available. Skipping comparison.\")\n",
    "    comparison = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Visualize Memory Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline_results:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(RANKS_TO_TEST))\n",
    "    width = 0.35\n",
    "    \n",
    "    lora_data = baseline_df.sort_values('rank')\n",
    "    qlora_data = qlora_df.sort_values('rank')\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, lora_data['peak_memory_mb'], width, \n",
    "                   label='LoRA (16-bit)', color='#3498db', alpha=0.8, edgecolor='black')\n",
    "    bars2 = ax.bar(x + width/2, qlora_data['peak_memory_mb'], width,\n",
    "                   label='QLoRA (4-bit)', color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    ax.set_xlabel('LoRA Rank (r)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Peak GPU Memory (MB)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Memory Usage Comparison: LoRA vs QLoRA (Unsloth Optimized)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'r={r}' for r in RANKS_TO_TEST])\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.0f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.0f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('../results/figures', exist_ok=True)\n",
    "    plt.savefig('../results/figures/memory_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Training Efficiency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline_results:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Time per step\n",
    "    x = np.arange(len(RANKS_TO_TEST))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, baseline_df['time_per_step'], width, \n",
    "            label='LoRA (16-bit)', color='#3498db', alpha=0.8, edgecolor='black')\n",
    "    ax1.bar(x + width/2, qlora_df['time_per_step'], width,\n",
    "            label='QLoRA (4-bit)', color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "    ax1.set_xlabel('Rank', fontweight='bold')\n",
    "    ax1.set_ylabel('Time per Step (s)', fontweight='bold')\n",
    "    ax1.set_title('Training Speed Comparison (Unsloth)')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([f'r={r}' for r in RANKS_TO_TEST])\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Training loss\n",
    "    ax2.plot(baseline_df['rank'], baseline_df['training_loss'], \n",
    "             marker='o', linewidth=2.5, markersize=10, label='LoRA (16-bit)', color='#3498db')\n",
    "    ax2.plot(qlora_df['rank'], qlora_df['training_loss'],\n",
    "             marker='s', linewidth=2.5, markersize=10, label='QLoRA (4-bit)', color='#e74c3c')\n",
    "    ax2.set_xlabel('Rank', fontweight='bold')\n",
    "    ax2.set_ylabel('Training Loss', fontweight='bold')\n",
    "    ax2.set_title('Training Loss Comparison')\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "    ax2.set_xticks(RANKS_TO_TEST)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/figures/training_efficiency.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate speedup\n",
    "    avg_speedup = baseline_df['time_per_step'].mean() / qlora_df['time_per_step'].mean()\n",
    "    print(f\"\\n‚ö° Average training speed: {avg_speedup:.2f}x (QLoRA vs LoRA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings\n",
    "\n",
    "### Fill in after running experiments:\n",
    "\n",
    "**Memory Reduction (with Unsloth optimization):**\n",
    "- Average reduction: _____%\n",
    "- Rank 8: LoRA _____ MB ‚Üí QLoRA _____ MB\n",
    "\n",
    "**Performance:**\n",
    "- Training loss comparable: [YES/NO]\n",
    "- Time per step: [FASTER/SLOWER/SIMILAR]\n",
    "- Loss difference at r=8: ______\n",
    "\n",
    "**Unsloth Benefits Observed:**\n",
    "- Speedup vs baseline: _____x\n",
    "- Additional memory savings: _____%\n",
    "\n",
    "**Observations:**\n",
    "- [Document trends]\n",
    "- [Note any unexpected behavior]\n",
    "- [Compare to theoretical expectations]\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "- Proceed to Part 3: Diagnostic analysis (hypothesis testing, failure modes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save QLoRA results\n",
    "with open(f\"{OUTPUT_DIR}/qlora_results.pkl\", 'wb') as f:\n",
    "    pickle.dump(qlora_results_list, f)\n",
    "\n",
    "print(f\"‚úì Results saved to {OUTPUT_DIR}/qlora_results.pkl\")\n",
    "print(f\"‚úì CSV saved to {OUTPUT_DIR}/qlora_results.csv\")\n",
    "if comparison is not None:\n",
    "    print(f\"‚úì Comparison saved to ../results/tables/memory_comparison.csv\")\n",
    "print(f\"‚úì Plots saved to ../results/figures/\")\n",
    "print(\"\\nüéâ QLoRA experiments complete!\")\n",
    "print(\"üìù Proceed to notebook 03_diagnostic_analysis.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
