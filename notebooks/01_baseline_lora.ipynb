{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Diagnostic Analysis - Part 1: Baseline LoRA (16-bit) with Unsloth\n",
    "\n",
    "## Objective\n",
    "Establish baseline performance using standard LoRA with 16-bit precision on GPT-2 Medium (355M parameters) using **Unsloth** library (as recommended by TA).\n",
    "\n",
    "## Key Questions\n",
    "1. What is the memory requirement for 16-bit LoRA fine-tuning?\n",
    "2. How does performance scale with different ranks (r ‚àà {2, 4, 8, 16})?\n",
    "3. What is the training efficiency (time per step)?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "### 1.1 Install Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth - optimized LoRA/QLoRA library\n",
    "import torch\n",
    "\n",
    "# Check CUDA version\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "print(f\"GPU Compute Capability: {major_version}.{minor_version}\")\n",
    "\n",
    "# Install Unsloth\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# Additional dependencies\n",
    "!pip install -q datasets matplotlib seaborn pandas numpy scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Import Unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Add src to path (upload src/ folder to Colab first)\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom modules with clean names\n",
    "from model_utils import load_gpt2_unsloth, setup_gpt2_lora, clear_memory, get_model_memory_usage\n",
    "from training import prepare_alpaca_dataset, run_experiment_unsloth\n",
    "from visualization import create_results_table\n",
    "\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental configuration\n",
    "MODEL_NAME = \"gpt2-medium\"  # 355M parameters\n",
    "NUM_SAMPLES = 1000  # Small dataset for quick diagnostic experiments\n",
    "MAX_STEPS = 200  # Training steps per experiment\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# Ranks to test\n",
    "RANKS_TO_TEST = [2, 4, 8, 16]\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"./results_baseline_lora\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Library: Unsloth (optimized)\")\n",
    "print(f\"  Quantization: 16-bit (baseline)\")\n",
    "print(f\"  Training samples: {NUM_SAMPLES}\")\n",
    "print(f\"  Max steps: {MAX_STEPS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Ranks to test: {RANKS_TO_TEST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Baseline LoRA Experiments\n",
    "\n",
    "We'll train LoRA (16-bit) with different ranks using Unsloth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "results_list = []\n",
    "\n",
    "for rank in RANKS_TO_TEST:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running LoRA (16-bit) with rank r={rank} using Unsloth\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    try:\n",
    "        result, model, tokenizer = run_experiment_unsloth(\n",
    "            model_name=MODEL_NAME,\n",
    "            load_in_4bit=False,  # 16-bit LoRA\n",
    "            rank=rank,\n",
    "            num_samples=NUM_SAMPLES,\n",
    "            max_steps=MAX_STEPS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            output_dir=OUTPUT_DIR\n",
    "        )\n",
    "        \n",
    "        results_list.append(result)\n",
    "        \n",
    "        # Clean up to free memory\n",
    "        del model\n",
    "        del tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with rank {rank}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n‚úì All experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Analysis\n",
    "\n",
    "### 4.1 Create Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results table\n",
    "results_df = create_results_table(\n",
    "    results_list,\n",
    "    save_path=f\"{OUTPUT_DIR}/baseline_lora_results.csv\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìä BASELINE LoRA RESULTS (Unsloth)\")\n",
    "print(\"=\"*80)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Memory Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot memory usage by rank\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(results_df['rank'], results_df['peak_memory_mb'], color='#3498db', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('LoRA Rank (r)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Peak GPU Memory (MB)', fontsize=12, fontweight='bold')\n",
    "plt.title('Baseline LoRA (16-bit) with Unsloth: Memory Usage by Rank', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(results_df['rank'])\n",
    "\n",
    "# Add value labels on bars\n",
    "for idx, row in results_df.iterrows():\n",
    "    plt.text(row['rank'], row['peak_memory_mb'] + 50, \n",
    "             f\"{row['peak_memory_mb']:.0f}\", \n",
    "             ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/baseline_memory_by_rank.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Memory Statistics:\")\n",
    "print(f\"  Average: {results_df['peak_memory_mb'].mean():.2f} MB\")\n",
    "print(f\"  Min (r={results_df.loc[results_df['peak_memory_mb'].idxmin(), 'rank']:.0f}): {results_df['peak_memory_mb'].min():.2f} MB\")\n",
    "print(f\"  Max (r={results_df.loc[results_df['peak_memory_mb'].idxmax(), 'rank']:.0f}): {results_df['peak_memory_mb'].max():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dual plot: time per step and training loss\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time per step\n",
    "ax1.bar(results_df['rank'], results_df['time_per_step'], color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "ax1.set_xlabel('LoRA Rank (r)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Time per Step (seconds)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Training Speed by Rank', fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_xticks(results_df['rank'])\n",
    "\n",
    "# Training loss\n",
    "ax2.plot(results_df['rank'], results_df['training_loss'], \n",
    "         marker='o', linewidth=2.5, markersize=10, color='#e74c3c', alpha=0.8)\n",
    "ax2.set_xlabel('LoRA Rank (r)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Training Loss', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Training Loss by Rank', fontweight='bold')\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_xticks(results_df['rank'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/baseline_efficiency.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚ö° Training Efficiency:\")\n",
    "print(f\"  Average time per step: {results_df['time_per_step'].mean():.3f}s\")\n",
    "print(f\"  Average training loss: {results_df['training_loss'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Findings\n",
    "\n",
    "### Results Summary\n",
    "\n",
    "Fill in after running experiments:\n",
    "\n",
    "**Memory Usage (Unsloth-optimized):**\n",
    "- Rank 2: ______ MB\n",
    "- Rank 4: ______ MB\n",
    "- Rank 8: ______ MB\n",
    "- Rank 16: ______ MB\n",
    "\n",
    "**Training Speed:**\n",
    "- Average time per step: ______ s\n",
    "\n",
    "**Training Loss:**\n",
    "- Best rank (lowest loss): r = ______\n",
    "- Loss at r=8: ______\n",
    "\n",
    "**Observations:**\n",
    "- [Document trends - does memory scale linearly with rank?]\n",
    "- [Note Unsloth performance - is it faster than expected?]\n",
    "- [Identify optimal rank for baseline comparison]\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "- Proceed to Part 2: Implement QLoRA (4-bit) with Unsloth and compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results for Next Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for comparison in subsequent notebooks\n",
    "with open(f\"{OUTPUT_DIR}/baseline_results.pkl\", 'wb') as f:\n",
    "    pickle.dump(results_list, f)\n",
    "\n",
    "print(f\"‚úì Results saved to {OUTPUT_DIR}/baseline_results.pkl\")\n",
    "print(f\"‚úì CSV saved to {OUTPUT_DIR}/baseline_lora_results.csv\")\n",
    "print(f\"‚úì Plots saved to {OUTPUT_DIR}/\")\n",
    "print(\"\\nüéâ Baseline LoRA experiments complete!\")\n",
    "print(\"üìù Proceed to notebook 02_qlora_implementation.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
