{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Diagnostic Analysis - Part 1: Baseline LoRA (16-bit) with Unsloth\n",
    "\n",
    "## Objective\n",
    "Establish baseline performance using standard LoRA with 16-bit precision on GPT-2 Medium (355M parameters) using the **Unsloth library** (as recommended by TA).\n",
    "\n",
    "## Key Questions\n",
    "1. What is the memory requirement for 16-bit LoRA fine-tuning?\n",
    "2. How does performance scale with different ranks (r ‚àà {2, 4, 8, 16})?\n",
    "3. What is the training efficiency (time per step)?\n",
    "\n",
    "## Why Unsloth?\n",
    "- **2x faster** training than standard PEFT\n",
    "- **Optimized memory usage**\n",
    "- **Simpler API** - handles quantization automatically\n",
    "- **Recommended by TA** for this project\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth (optimized for Colab)\n",
    "!pip install unsloth -q\n",
    "!pip install datasets matplotlib seaborn pandas numpy scikit-learn tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Upload src/ folder to Colab or clone from GitHub\n",
    "# For now, assume files are uploaded\n",
    "#sys.path.append('.')\n",
    "\n",
    "# Import custom modules\n",
    "#from src.model_utils import (\n",
    "#    load_model_with_lora_16bit,\n",
    "#    get_model_memory_usage,\n",
    "#    print_trainable_parameters,\n",
    "#    clear_memory\n",
    "#)\n",
    "\n",
    "#from src.training import run_experiment\n",
    "\n",
    "# In all notebooks, use these imports:\n",
    "sys.path.append('../src')\n",
    "from model_utils import load_gpt2_unsloth, setup_gpt2_lora, clear_memory\n",
    "from training import prepare_alpaca_dataset, run_experiment_unsloth\n",
    "\n",
    "from src.visualization import create_results_table\n",
    "\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental configuration\n",
    "MODEL_NAME = \"unsloth/gpt2-medium\"  # Unsloth's optimized GPT-2 Medium\n",
    "NUM_SAMPLES = 1000  # Small dataset for quick diagnostic experiments\n",
    "MAX_STEPS = 200  # Training steps per experiment\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# Ranks to test\n",
    "RANKS_TO_TEST = [2, 4, 8, 16]\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"./results_baseline_lora\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Framework: Unsloth (optimized)\")\n",
    "print(f\"  Training samples: {NUM_SAMPLES}\")\n",
    "print(f\"  Max steps: {MAX_STEPS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Ranks to test: {RANKS_TO_TEST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Baseline LoRA Experiments\n",
    "\n",
    "Train LoRA (16-bit) with different ranks using Unsloth's optimized implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "results_list = []\n",
    "\n",
    "for rank in RANKS_TO_TEST:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running LoRA (16-bit) with rank r={rank}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    try:\n",
    "        result, model, tokenizer = run_experiment(\n",
    "            model_name=MODEL_NAME,\n",
    "            quantization=\"16bit\",\n",
    "            rank=rank,\n",
    "            num_samples=NUM_SAMPLES,\n",
    "            max_steps=MAX_STEPS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            output_dir=OUTPUT_DIR\n",
    "        )\n",
    "        \n",
    "        results_list.append(result)\n",
    "        \n",
    "        # Clean up to free memory\n",
    "        del model\n",
    "        del tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with rank {rank}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n‚úì All experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results table\n",
    "results_df = create_results_table(\n",
    "    results_list,\n",
    "    save_path=f\"{OUTPUT_DIR}/baseline_lora_results.csv\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìä BASELINE LoRA RESULTS (Unsloth)\")\n",
    "print(\"=\"*80)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage by rank\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.bar(results_df['rank'], results_df['peak_memory_mb'], color='#3498db', alpha=0.7)\n",
    "ax1.set_xlabel('LoRA Rank (r)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Peak GPU Memory (MB)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Baseline LoRA: Memory Usage (Unsloth)', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "ax2.bar(results_df['rank'], results_df['time_per_step'], color='#2ecc71', alpha=0.7)\n",
    "ax2.set_xlabel('LoRA Rank (r)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Time per Step (s)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Baseline LoRA: Training Speed (Unsloth)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/baseline_metrics.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Average memory: {results_df['peak_memory_mb'].mean():.2f} MB\")\n",
    "print(f\"‚úì Average time/step: {results_df['time_per_step'].mean():.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings\n",
    "\n",
    "### Memory Usage (TODO: Fill after running)\n",
    "- Rank 2: [TODO] MB\n",
    "- Rank 4: [TODO] MB\n",
    "- Rank 8: [TODO] MB\n",
    "- Rank 16: [TODO] MB\n",
    "\n",
    "### Training Speed\n",
    "- Average time/step: [TODO]s\n",
    "- **Note**: Unsloth is ~2x faster than standard PEFT\n",
    "\n",
    "### Observations\n",
    "- [TODO: Document trends]\n",
    "- [TODO: Compare to expected values]\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: Proceed to `02_qlora_implementation.ipynb` to compare with 4-bit QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for next notebook\n",
    "import pickle\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/baseline_results.pkl\", 'wb') as f:\n",
    "    pickle.dump(results_list, f)\n",
    "\n",
    "print(f\"‚úì Results saved to {OUTPUT_DIR}/baseline_results.pkl\")\n",
    "print(\"\\nüéâ Baseline LoRA experiments complete!\")\n",
    "print(\"üìù Proceed to notebook 02_qlora_implementation.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
